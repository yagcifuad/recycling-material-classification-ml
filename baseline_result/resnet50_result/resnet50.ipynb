{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e86b8f76-458c-4467-ab7c-b0b5826c77af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d499d4f-66c6-40d1-b764-746073a3aaf5",
   "metadata": {},
   "source": [
    "### ******** 1. SETUP ENVIRONMENT ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e7badb8-ffa4-4c47-96a5-725577f27826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0\n",
      "Using MPS device\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check device availability\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75bed56-c3df-4d16-8863-f7a768b24279",
   "metadata": {},
   "source": [
    "### ******** 2. CONFIGURATION SETTINGS ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec45020c-9a20-4bf2-8eaf-f594f4942855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATASET_PATH = \"../../archive/images/images\" \n",
    "RESULTS_DIR = \"resnet50_baseline_kaggle_results\"\n",
    "\n",
    "# DATASET_PATH = \"../../dataset-resized\" \n",
    "# RESULTS_DIR = \"resnet50_baseline_results\"\n",
    "\n",
    "# DATASET_PATH = \"../../garbage-dataset-2\"  \n",
    "# RESULTS_DIR = \"resnet50_baseline_garbage_dataset_results\"\n",
    "\n",
    "# Check if dataset path exists\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    raise FileNotFoundError(f\"Dataset path {DATASET_PATH} does not exist\")\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Define baseline hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "OPTIMIZER = 'Adam'\n",
    "EPOCHS = 30\n",
    "DROPOUT_RATE = 0.5\n",
    "UNFREEZE_LAYERS = 'none'\n",
    "NUM_WORKERS = 4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30b76d92-8a5c-4776-a105-3f350a8e4da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******** 3. DATA PREPARATION ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35ae2ac5-ff59-4190-90e8-df5615f02f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    \"\"\"Define image transformations for training and validation\"\"\"\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((240, 240)),\n",
    "        transforms.CenterCrop((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),  # Basic augmentation for better generalization\n",
    "        transforms.RandomRotation(10),      # Minor rotation augmentation\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "def load_data(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
    "    \"\"\"Load and prepare the dataset with train/val/test splits\"\"\"\n",
    "    train_transform, val_transform = get_transforms()\n",
    "    \n",
    "    try:\n",
    "        # Load the full dataset\n",
    "        full_dataset = datasets.ImageFolder(DATASET_PATH, transform=train_transform)\n",
    "        class_names = full_dataset.classes\n",
    "        num_classes = len(class_names)\n",
    "        \n",
    "        # Calculate sizes for splits (70% train, 20% val, 10% test)\n",
    "        total_size = len(full_dataset)\n",
    "        train_size = int(0.7 * total_size)\n",
    "        val_size = int(0.2 * total_size)\n",
    "        test_size = total_size - train_size - val_size\n",
    "        \n",
    "        # Split the dataset\n",
    "        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "            full_dataset, [train_size, val_size, test_size]\n",
    "        )\n",
    "        \n",
    "        # Apply validation transform to validation and test sets\n",
    "        val_dataset.dataset.transform = val_transform\n",
    "        test_dataset.dataset.transform = val_transform\n",
    "        \n",
    "        # Create data loaders with optimized settings\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if device.type != 'cpu' else False\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if device.type != 'cpu' else False\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if device.type != 'cpu' else False\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset loaded: {len(train_dataset)} training, {len(val_dataset)} validation, {len(test_dataset)} test images\")\n",
    "        print(f\"Classes: {class_names}\")\n",
    "        \n",
    "        # Print dataset distribution\n",
    "        class_counts = {class_name: 0 for class_name in class_names}\n",
    "        for _, class_idx in full_dataset.samples:\n",
    "            class_counts[class_names[class_idx]] += 1\n",
    "            \n",
    "        print(\"Class distribution:\")\n",
    "        for class_name, count in class_counts.items():\n",
    "            print(f\"  {class_name}: {count} images ({count/total_size:.1%})\")\n",
    "        \n",
    "        return train_loader, val_loader, test_loader, class_names, num_classes\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4937a776-0e29-4dba-836e-dcb1599101ad",
   "metadata": {},
   "source": [
    "### ******** 4. MODEL ARCHITECTURE ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77d6de11-69c3-441e-b2bd-6bb45bd09c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_resnet50(num_classes, dropout_rate=DROPOUT_RATE):\n",
    "    \"\"\"Create ResNet-50 model with pretrained weights\"\"\"\n",
    "    try:\n",
    "        # Load pretrained model\n",
    "        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        \n",
    "        # Freeze all parameters for baseline model\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Replace the fully connected layer\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(num_features, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Always unfreeze the fully connected layer\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # If specified, unfreeze additional layers based on the strategy\n",
    "        if UNFREEZE_LAYERS == 'last_block':\n",
    "            # Unfreeze the last layer (layer4) for ResNet50\n",
    "            for param in model.layer4.parameters():\n",
    "                param.requires_grad = True\n",
    "        elif UNFREEZE_LAYERS == 'all':\n",
    "            # Unfreeze all layers\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating model: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cd62fb-15aa-44a4-adba-80ec5a1d4afd",
   "metadata": {},
   "source": [
    "### ******** 5. TRAINING SETUP ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07f35ff5-762e-43c5-98a9-9dcc2dd12a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    def __init__(self, patience=7, verbose=True, path='best_model.pth', delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.path = path\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "    \n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd963877-e2c7-4bcb-8593-6bf967a77e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******** 6. TRAINING FUNCTION ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "851436e8-b128-4401-a481-6e7e9d8e7a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=EPOCHS, path='model.pth'):\n",
    "    \"\"\"Train the model and track performance metrics\"\"\"\n",
    "    early_stopping = EarlyStopping(patience=7, path=path)\n",
    "    \n",
    "    # History tracking\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [], \n",
    "        'train_acc': [], 'val_acc': [], \n",
    "        'f1': []\n",
    "    }\n",
    "    \n",
    "    model = model.to(device)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * batch_size\n",
    "            running_corrects += torch.sum(preds == labels.data).item()\n",
    "            total_samples += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = running_corrects / total_samples\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc)\n",
    "        \n",
    "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total_samples = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=f\"Validation\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                batch_size = inputs.size(0)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item() * batch_size\n",
    "                running_corrects += torch.sum(preds == labels.data).item()\n",
    "                total_samples += batch_size\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = running_corrects / total_samples\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        history['val_loss'].append(epoch_loss)\n",
    "        history['val_acc'].append(epoch_acc)\n",
    "        history['f1'].append(f1)\n",
    "        \n",
    "        print(f'Val Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} F1: {f1:.4f}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        early_stopping(epoch_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    # Calculate training time\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print(f'Training completed in {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s')\n",
    "    \n",
    "    # Load best model\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(early_stopping.path))\n",
    "        print(f\"Loaded best model from {early_stopping.path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load best model - {e}\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de7e3afc-2a5f-4524-a51c-c68265cdb277",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ******** 7. EVALUATION FUNCTION ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19a53dfa-fefb-4cc1-aa34-c3f12e9c0d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, class_names, save_path=None):\n",
    "    \"\"\"Evaluate the model and generate detailed metrics and visualizations\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
    "    report_str = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "    \n",
    "    # Print report\n",
    "    print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "    print('\\nClassification Report:')\n",
    "    print(report_str)\n",
    "    \n",
    "    if save_path:\n",
    "        # Save detailed report\n",
    "        with open(f\"{save_path}_report.txt\", 'w') as f:\n",
    "            f.write(f'Validation Accuracy: {accuracy:.4f}\\n\\n')\n",
    "            f.write(report_str)\n",
    "            \n",
    "        # Save report as JSON for further analysis\n",
    "        with open(f\"{save_path}_report.json\", 'w') as f:\n",
    "            json.dump(report, f, indent=4)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{save_path}_confusion_matrix.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot normalized confusion matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Normalized Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{save_path}_confusion_matrix_norm.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        print(f'Evaluation results saved to {save_path}')\n",
    "    \n",
    "    return accuracy, report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea058b7-22d2-4a99-aef0-73ccf51937b9",
   "metadata": {},
   "source": [
    "### ******** 8. VISUALIZATION FUNCTION ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85b669ec-9c60-4f80-bbac-b876320e487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, save_path=None):\n",
    "    \"\"\"Plot training metrics history\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Acc')\n",
    "    plt.plot(history['val_acc'], label='Val Acc')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history['f1'], label='F1 Score')\n",
    "    plt.title('F1 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}_history.png\")\n",
    "        print(f'Training history plot saved to {save_path}_history.png')\n",
    "    \n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38ceef9-4623-41f9-8160-4ae9e89e6df2",
   "metadata": {},
   "source": [
    "### ******** 9. MAIN EXECUTION FUNCTION ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a67ec24e-c7ef-484a-a485-41d4c4e0fcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ResNet-50 Baseline Model for Recycling Material Classification\n",
      "==================================================\n",
      "Dataset loaded: 10500 training, 3000 validation, 1500 test images\n",
      "Classes: ['aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging', 'clothing', 'coffee_grounds', 'disposable_plastic_cutlery', 'eggshells', 'food_waste', 'glass_beverage_bottles', 'glass_cosmetic_containers', 'glass_food_jars', 'magazines', 'newspaper', 'office_paper', 'paper_cups', 'plastic_cup_lids', 'plastic_detergent_bottles', 'plastic_food_containers', 'plastic_shopping_bags', 'plastic_soda_bottles', 'plastic_straws', 'plastic_trash_bags', 'plastic_water_bottles', 'shoes', 'steel_food_cans', 'styrofoam_cups', 'styrofoam_food_containers', 'tea_bags']\n",
      "Class distribution:\n",
      "  aerosol_cans: 500 images (3.3%)\n",
      "  aluminum_food_cans: 500 images (3.3%)\n",
      "  aluminum_soda_cans: 500 images (3.3%)\n",
      "  cardboard_boxes: 500 images (3.3%)\n",
      "  cardboard_packaging: 500 images (3.3%)\n",
      "  clothing: 500 images (3.3%)\n",
      "  coffee_grounds: 500 images (3.3%)\n",
      "  disposable_plastic_cutlery: 500 images (3.3%)\n",
      "  eggshells: 500 images (3.3%)\n",
      "  food_waste: 500 images (3.3%)\n",
      "  glass_beverage_bottles: 500 images (3.3%)\n",
      "  glass_cosmetic_containers: 500 images (3.3%)\n",
      "  glass_food_jars: 500 images (3.3%)\n",
      "  magazines: 500 images (3.3%)\n",
      "  newspaper: 500 images (3.3%)\n",
      "  office_paper: 500 images (3.3%)\n",
      "  paper_cups: 500 images (3.3%)\n",
      "  plastic_cup_lids: 500 images (3.3%)\n",
      "  plastic_detergent_bottles: 500 images (3.3%)\n",
      "  plastic_food_containers: 500 images (3.3%)\n",
      "  plastic_shopping_bags: 500 images (3.3%)\n",
      "  plastic_soda_bottles: 500 images (3.3%)\n",
      "  plastic_straws: 500 images (3.3%)\n",
      "  plastic_trash_bags: 500 images (3.3%)\n",
      "  plastic_water_bottles: 500 images (3.3%)\n",
      "  shoes: 500 images (3.3%)\n",
      "  steel_food_cans: 500 images (3.3%)\n",
      "  styrofoam_cups: 500 images (3.3%)\n",
      "  styrofoam_food_containers: 500 images (3.3%)\n",
      "  tea_bags: 500 images (3.3%)\n",
      "Model created: ResNet-50 with 30 output classes\n",
      "Total parameters: 23,569,502\n",
      "Trainable parameters: 61,470 (0.26%)\n",
      "Training strategy: Only fully connected layer trained, backbone frozen\n",
      "\n",
      "Starting model training...\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:43<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6008 Acc: 0.6188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9564 Acc: 0.7497 F1: 0.7448\n",
      "Validation loss decreased (inf --> 0.956367). Saving model...\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:40<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8574 Acc: 0.7635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7608 Acc: 0.7880 F1: 0.7814\n",
      "Validation loss decreased (0.956367 --> 0.760819). Saving model...\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:40<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7078 Acc: 0.7955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6668 Acc: 0.8047 F1: 0.8013\n",
      "Validation loss decreased (0.760819 --> 0.666751). Saving model...\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:40<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6326 Acc: 0.8119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6567 Acc: 0.7960 F1: 0.7940\n",
      "Validation loss decreased (0.666751 --> 0.656736). Saving model...\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:40<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5641 Acc: 0.8340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6017 Acc: 0.8130 F1: 0.8103\n",
      "Validation loss decreased (0.656736 --> 0.601721). Saving model...\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:39<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5409 Acc: 0.8322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5644 Acc: 0.8217 F1: 0.8209\n",
      "Validation loss decreased (0.601721 --> 0.564433). Saving model...\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:40<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4947 Acc: 0.8488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5764 Acc: 0.8203 F1: 0.8159\n",
      "EarlyStopping counter: 1 out of 7\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:40<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4746 Acc: 0.8509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5609 Acc: 0.8257 F1: 0.8233\n",
      "Validation loss decreased (0.564433 --> 0.560909). Saving model...\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:39<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4548 Acc: 0.8567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5385 Acc: 0.8320 F1: 0.8298\n",
      "Validation loss decreased (0.560909 --> 0.538482). Saving model...\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:40<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4386 Acc: 0.8642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5294 Acc: 0.8333 F1: 0.8324\n",
      "Validation loss decreased (0.538482 --> 0.529363). Saving model...\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:40<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4301 Acc: 0.8604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5273 Acc: 0.8250 F1: 0.8227\n",
      "Validation loss decreased (0.529363 --> 0.527319). Saving model...\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:40<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4117 Acc: 0.8686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5287 Acc: 0.8287 F1: 0.8270\n",
      "EarlyStopping counter: 1 out of 7\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:40<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4033 Acc: 0.8673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5174 Acc: 0.8320 F1: 0.8300\n",
      "Validation loss decreased (0.527319 --> 0.517392). Saving model...\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:39<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3913 Acc: 0.8711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5268 Acc: 0.8290 F1: 0.8272\n",
      "EarlyStopping counter: 1 out of 7\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:39<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3955 Acc: 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5082 Acc: 0.8350 F1: 0.8335\n",
      "Validation loss decreased (0.517392 --> 0.508207). Saving model...\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:41<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3799 Acc: 0.8722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:43<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5467 Acc: 0.8290 F1: 0.8270\n",
      "EarlyStopping counter: 1 out of 7\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:41<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3764 Acc: 0.8738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:43<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5281 Acc: 0.8360 F1: 0.8344\n",
      "EarlyStopping counter: 2 out of 7\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:41<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3819 Acc: 0.8714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:43<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5054 Acc: 0.8347 F1: 0.8330\n",
      "Validation loss decreased (0.508207 --> 0.505373). Saving model...\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:41<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3761 Acc: 0.8705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:43<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5239 Acc: 0.8320 F1: 0.8310\n",
      "EarlyStopping counter: 1 out of 7\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:41<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3582 Acc: 0.8779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:43<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5043 Acc: 0.8347 F1: 0.8335\n",
      "Validation loss decreased (0.505373 --> 0.504312). Saving model...\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:40<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3571 Acc: 0.8797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5068 Acc: 0.8397 F1: 0.8373\n",
      "EarlyStopping counter: 1 out of 7\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:40<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3473 Acc: 0.8816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5064 Acc: 0.8353 F1: 0.8337\n",
      "EarlyStopping counter: 2 out of 7\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:40<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3585 Acc: 0.8754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5159 Acc: 0.8350 F1: 0.8338\n",
      "EarlyStopping counter: 3 out of 7\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:40<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3523 Acc: 0.8771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5309 Acc: 0.8333 F1: 0.8315\n",
      "EarlyStopping counter: 4 out of 7\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:40<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3475 Acc: 0.8817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5011 Acc: 0.8370 F1: 0.8368\n",
      "Validation loss decreased (0.504312 --> 0.501143). Saving model...\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:40<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3409 Acc: 0.8830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5218 Acc: 0.8357 F1: 0.8348\n",
      "EarlyStopping counter: 1 out of 7\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:40<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3424 Acc: 0.8798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5168 Acc: 0.8393 F1: 0.8375\n",
      "EarlyStopping counter: 2 out of 7\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:40<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3383 Acc: 0.8815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5430 Acc: 0.8310 F1: 0.8303\n",
      "EarlyStopping counter: 3 out of 7\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:40<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3471 Acc: 0.8831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5088 Acc: 0.8390 F1: 0.8377\n",
      "EarlyStopping counter: 4 out of 7\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 329/329 [01:40<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3318 Acc: 0.8841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5227 Acc: 0.8380 F1: 0.8367\n",
      "EarlyStopping counter: 5 out of 7\n",
      "Training completed in 71m 47s\n",
      "Loaded best model from resnet50_baseline_kaggle_results/baseline/best_model.pth\n",
      "Training history plot saved to resnet50_baseline_kaggle_results/baseline/training_history_history.png\n",
      "\n",
      "Evaluating on validation set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████| 94/94 [00:42<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8370\n",
      "\n",
      "Classification Report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "              aerosol_cans       0.93      0.91      0.92       109\n",
      "        aluminum_food_cans       0.45      0.46      0.45        96\n",
      "        aluminum_soda_cans       0.82      0.81      0.82       107\n",
      "           cardboard_boxes       0.57      0.68      0.62        92\n",
      "       cardboard_packaging       0.62      0.56      0.59       106\n",
      "                  clothing       0.86      0.92      0.89       102\n",
      "            coffee_grounds       0.93      0.97      0.95       114\n",
      "disposable_plastic_cutlery       0.98      0.99      0.98        84\n",
      "                 eggshells       0.92      0.94      0.93        98\n",
      "                food_waste       0.91      0.96      0.93        90\n",
      "    glass_beverage_bottles       0.86      0.92      0.89       108\n",
      " glass_cosmetic_containers       0.88      0.87      0.87        91\n",
      "           glass_food_jars       0.94      0.87      0.90       101\n",
      "                 magazines       0.88      0.90      0.89        89\n",
      "                 newspaper       0.87      0.83      0.85        89\n",
      "              office_paper       0.86      0.74      0.79        96\n",
      "                paper_cups       0.81      0.85      0.83       108\n",
      "          plastic_cup_lids       0.89      0.80      0.85       102\n",
      " plastic_detergent_bottles       0.94      0.94      0.94       105\n",
      "   plastic_food_containers       0.84      0.84      0.84        91\n",
      "     plastic_shopping_bags       0.93      0.89      0.91       110\n",
      "      plastic_soda_bottles       0.78      0.76      0.77       103\n",
      "            plastic_straws       0.87      0.97      0.92        90\n",
      "        plastic_trash_bags       0.89      0.93      0.91       117\n",
      "     plastic_water_bottles       0.80      0.79      0.79       118\n",
      "                     shoes       0.96      0.94      0.95        87\n",
      "           steel_food_cans       0.57      0.55      0.56       121\n",
      "            styrofoam_cups       0.92      0.84      0.88       100\n",
      " styrofoam_food_containers       0.92      0.92      0.92        87\n",
      "                  tea_bags       0.81      0.84      0.82        89\n",
      "\n",
      "                  accuracy                           0.84      3000\n",
      "                 macro avg       0.84      0.84      0.84      3000\n",
      "              weighted avg       0.84      0.84      0.84      3000\n",
      "\n",
      "Evaluation results saved to resnet50_baseline_kaggle_results/baseline/validation_results\n",
      "\n",
      "Evaluating on test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████| 47/47 [00:33<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8547\n",
      "\n",
      "Classification Report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "              aerosol_cans       0.88      0.95      0.91        55\n",
      "        aluminum_food_cans       0.54      0.53      0.53        51\n",
      "        aluminum_soda_cans       0.76      0.91      0.83        45\n",
      "           cardboard_boxes       0.61      0.72      0.66        54\n",
      "       cardboard_packaging       0.63      0.52      0.57        50\n",
      "                  clothing       0.89      0.89      0.89        46\n",
      "            coffee_grounds       0.90      0.98      0.94        53\n",
      "disposable_plastic_cutlery       0.98      0.90      0.94        52\n",
      "                 eggshells       0.94      0.94      0.94        47\n",
      "                food_waste       0.98      1.00      0.99        45\n",
      "    glass_beverage_bottles       0.86      0.94      0.90        54\n",
      " glass_cosmetic_containers       1.00      0.91      0.95        44\n",
      "           glass_food_jars       0.90      0.84      0.87        32\n",
      "                 magazines       0.88      0.93      0.90        55\n",
      "                 newspaper       0.86      0.80      0.83        64\n",
      "              office_paper       0.86      0.79      0.82        53\n",
      "                paper_cups       0.79      0.86      0.82        43\n",
      "          plastic_cup_lids       0.94      0.81      0.87        54\n",
      " plastic_detergent_bottles       1.00      0.97      0.98        31\n",
      "   plastic_food_containers       0.83      0.91      0.87        44\n",
      "     plastic_shopping_bags       0.92      0.90      0.91        52\n",
      "      plastic_soda_bottles       0.84      0.87      0.86        55\n",
      "            plastic_straws       0.85      0.92      0.88        49\n",
      "        plastic_trash_bags       0.95      0.95      0.95        60\n",
      "     plastic_water_bottles       0.80      0.78      0.79        46\n",
      "                     shoes       1.00      0.96      0.98        50\n",
      "           steel_food_cans       0.61      0.55      0.58        49\n",
      "            styrofoam_cups       0.92      0.87      0.90        54\n",
      " styrofoam_food_containers       0.96      0.84      0.90        51\n",
      "                  tea_bags       0.86      0.92      0.89        62\n",
      "\n",
      "                  accuracy                           0.85      1500\n",
      "                 macro avg       0.86      0.86      0.86      1500\n",
      "              weighted avg       0.86      0.85      0.85      1500\n",
      "\n",
      "Evaluation results saved to resnet50_baseline_kaggle_results/baseline/test_results\n",
      "\n",
      "==================================================\n",
      "BASELINE MODEL SUMMARY\n",
      "==================================================\n",
      "Model: ResNet-50\n",
      "Classes: ['aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging', 'clothing', 'coffee_grounds', 'disposable_plastic_cutlery', 'eggshells', 'food_waste', 'glass_beverage_bottles', 'glass_cosmetic_containers', 'glass_food_jars', 'magazines', 'newspaper', 'office_paper', 'paper_cups', 'plastic_cup_lids', 'plastic_detergent_bottles', 'plastic_food_containers', 'plastic_shopping_bags', 'plastic_soda_bottles', 'plastic_straws', 'plastic_trash_bags', 'plastic_water_bottles', 'shoes', 'steel_food_cans', 'styrofoam_cups', 'styrofoam_food_containers', 'tea_bags']\n",
      "Total Parameters: 23,569,502\n",
      "Trainable Parameters: 61,470\n",
      "Validation Accuracy: 0.8370\n",
      "Test Accuracy: 0.8547\n",
      "Validation F1 Score: 0.8368\n",
      "Test F1 Score: 0.8540\n",
      "==================================================\n",
      "Full results saved to resnet50_baseline_kaggle_results/baseline\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ResNet-50 Baseline Model for Recycling Material Classification\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create directory for baseline results\n",
    "    baseline_dir = os.path.join(RESULTS_DIR, \"baseline\")\n",
    "    os.makedirs(baseline_dir, exist_ok=True)\n",
    "    \n",
    "    # Load data\n",
    "    train_loader, val_loader, test_loader, class_names, num_classes = load_data(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_resnet50(num_classes, dropout_rate=DROPOUT_RATE)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Model created: ResNet-50 with {num_classes} output classes\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%})\")\n",
    "    print(f\"Training strategy: {UNFREEZE_LAYERS if UNFREEZE_LAYERS != 'none' else 'Only fully connected layer trained, backbone frozen'}\")\n",
    "    \n",
    "    # Setup loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if OPTIMIZER == 'Adam':\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                              lr=LEARNING_RATE)\n",
    "    else:  # SGD as fallback\n",
    "        optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                             lr=LEARNING_RATE, momentum=0.9)\n",
    "    \n",
    "    # Save configuration \n",
    "    config = {\n",
    "        'model': 'ResNet-50',\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'optimizer': OPTIMIZER,\n",
    "        'epochs': EPOCHS,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'unfreeze_layers': UNFREEZE_LAYERS,\n",
    "        'num_workers': NUM_WORKERS,\n",
    "        'total_parameters': total_params,\n",
    "        'trainable_parameters': trainable_params,\n",
    "        'device': str(device),\n",
    "        'dataset_path': DATASET_PATH,\n",
    "        'num_classes': num_classes,\n",
    "        'class_names': class_names\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(baseline_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"\\nStarting model training...\")\n",
    "    model_path = os.path.join(baseline_dir, 'best_model.pth')\n",
    "    model, history = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, \n",
    "        num_epochs=EPOCHS, path=model_path\n",
    "    )\n",
    "    \n",
    "    # Plot and save training history\n",
    "    history_path = os.path.join(baseline_dir, 'training_history')\n",
    "    plot_history(history, save_path=history_path)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(\"\\nEvaluating on validation set:\")\n",
    "    val_results_path = os.path.join(baseline_dir, 'validation_results')\n",
    "    val_accuracy, val_report = evaluate_model(model, val_loader, class_names, save_path=val_results_path)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set:\")\n",
    "    test_results_path = os.path.join(baseline_dir, 'test_results')\n",
    "    test_accuracy, test_report = evaluate_model(model, test_loader, class_names, save_path=test_results_path)\n",
    "    \n",
    "    # Save model summary\n",
    "    model_summary = {\n",
    "        'model_type': 'ResNet-50',\n",
    "        'num_classes': num_classes,\n",
    "        'class_names': class_names,\n",
    "        'parameters': total_params,\n",
    "        'trainable_parameters': trainable_params,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'val_f1_score': val_report['weighted avg']['f1-score'],\n",
    "        'test_f1_score': test_report['weighted avg']['f1-score'],\n",
    "        'per_class_f1': {cls: test_report[cls]['f1-score'] for cls in class_names}\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(baseline_dir, 'model_summary.json'), 'w') as f:\n",
    "        json.dump(model_summary, f, indent=4)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"BASELINE MODEL SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Model: ResNet-50\")\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    print(f\"Total Parameters: {model_summary['parameters']:,}\")\n",
    "    print(f\"Trainable Parameters: {model_summary['trainable_parameters']:,}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Validation F1 Score: {val_report['weighted avg']['f1-score']:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_report['weighted avg']['f1-score']:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Full results saved to {baseline_dir}\")\n",
    "    \n",
    "    return model, test_accuracy, test_report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b695263-67b7-4b6a-8696-47f335d5ce09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca4500f-663f-4de7-bd6a-f32587c3f241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
