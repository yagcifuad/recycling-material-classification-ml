{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17df4980-5549-4def-bb74-01cfcda91240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0\n",
      "Using MPS device for feature extraction\n",
      "\n",
      "==================================================\n",
      "SVM Baseline Model for Recycling Material Classification\n",
      "==================================================\n",
      "Dataset loaded: 13833 training, 3952 validation, 1977 test images\n",
      "Classes: ['battery', 'biological', 'cardboard', 'clothes', 'glass', 'metal', 'paper', 'plastic', 'shoes', 'trash']\n",
      "Class distribution:\n",
      "  battery: 944 images (4.8%)\n",
      "  biological: 997 images (5.0%)\n",
      "  cardboard: 1825 images (9.2%)\n",
      "  clothes: 5327 images (27.0%)\n",
      "  glass: 3061 images (15.5%)\n",
      "  metal: 1020 images (5.2%)\n",
      "  paper: 1680 images (8.5%)\n",
      "  plastic: 1984 images (10.0%)\n",
      "  shoes: 1977 images (10.0%)\n",
      "  trash: 947 images (4.8%)\n",
      "Feature extractor created: resnet50\n",
      "\n",
      "Extracting features from training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|████████████████████| 433/433 [01:53<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (13833, 2048), Labels shape: (13833,)\n",
      "\n",
      "Extracting features from validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|████████████████████| 124/124 [00:49<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation features shape: (3952, 2048), Labels shape: (3952,)\n",
      "\n",
      "Extracting features from test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████████████████| 62/62 [00:36<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test features shape: (1977, 2048), Labels shape: (1977,)\n",
      "\n",
      "Training SVM model...\n",
      "Training SVM with C=1.0, kernel=rbf, gamma=scale\n",
      "SVM training completed in 152.80 seconds\n",
      "SVM model saved to svm_baseline_garbage_dataset_results/baseline/svm_model.pkl\n",
      "\n",
      "Evaluating on validation set:\n",
      "Accuracy: 0.9638, F1 Score: 0.9638\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     battery       0.97      0.96      0.96       182\n",
      "  biological       0.97      0.99      0.98       206\n",
      "   cardboard       0.95      0.96      0.95       364\n",
      "     clothes       1.00      0.99      0.99      1045\n",
      "       glass       0.95      0.98      0.97       613\n",
      "       metal       0.92      0.96      0.94       206\n",
      "       paper       0.94      0.94      0.94       319\n",
      "     plastic       0.97      0.91      0.94       430\n",
      "       shoes       0.97      0.99      0.98       425\n",
      "       trash       0.88      0.87      0.87       162\n",
      "\n",
      "    accuracy                           0.96      3952\n",
      "   macro avg       0.95      0.95      0.95      3952\n",
      "weighted avg       0.96      0.96      0.96      3952\n",
      "\n",
      "Evaluation results saved to svm_baseline_garbage_dataset_results/baseline/validation_results\n",
      "\n",
      "Evaluating on test set:\n",
      "Accuracy: 0.9641, F1 Score: 0.9641\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     battery       0.96      0.97      0.96        93\n",
      "  biological       0.98      0.98      0.98       113\n",
      "   cardboard       0.95      0.95      0.95       170\n",
      "     clothes       0.99      0.99      0.99       528\n",
      "       glass       0.96      0.97      0.97       313\n",
      "       metal       0.93      0.92      0.93        90\n",
      "       paper       0.95      0.94      0.94       171\n",
      "     plastic       0.96      0.91      0.94       217\n",
      "       shoes       0.97      0.98      0.98       188\n",
      "       trash       0.89      0.96      0.92        94\n",
      "\n",
      "    accuracy                           0.96      1977\n",
      "   macro avg       0.95      0.96      0.96      1977\n",
      "weighted avg       0.96      0.96      0.96      1977\n",
      "\n",
      "Evaluation results saved to svm_baseline_garbage_dataset_results/baseline/test_results\n",
      "\n",
      "==================================================\n",
      "BASELINE MODEL SUMMARY\n",
      "==================================================\n",
      "Model: SVM with resnet50 features\n",
      "Classes: ['battery', 'biological', 'cardboard', 'clothes', 'glass', 'metal', 'paper', 'plastic', 'shoes', 'trash']\n",
      "Parameters: C=1.0, kernel=rbf, gamma=scale\n",
      "Feature extractor: resnet50\n",
      "Feature dimension: 2048\n",
      "Validation Accuracy: 0.9638\n",
      "Test Accuracy: 0.9641\n",
      "Validation F1 Score: 0.9638\n",
      "Test F1 Score: 0.9641\n",
      "==================================================\n",
      "Full results saved to svm_baseline_garbage_dataset_results/baseline\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# ******** 1. SETUP ENVIRONMENT ********\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check device availability for feature extraction\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device for feature extraction\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using CUDA device for feature extraction: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device for feature extraction\")\n",
    "\n",
    "# ******** 2. CONFIGURATION SETTINGS ********\n",
    "# Define paths\n",
    "DATASET_PATH = \"../../garbage-dataset-2\" \n",
    "RESULTS_DIR = \"svm_resnet50_result/garbage_dataset\"\n",
    "\n",
    "# DATASET_PATH = \"../../archive/images/images\"  \n",
    "# RESULTS_DIR = \"svm_baseline_kaggle_dataset_results\"\n",
    "\n",
    "# DATASET_PATH = \"../../dataset-resized\" \n",
    "# RESULTS_DIR = \"svm_baseline_results\"\n",
    "\n",
    "# Check if dataset path exists\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    raise FileNotFoundError(f\"Dataset path {DATASET_PATH} does not exist\")\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Define baseline hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "FEATURE_EXTRACTOR = 'resnet50'  # Using ResNet-50 as feature extractor\n",
    "C = 1.0  # Regularization parameter\n",
    "KERNEL = 'rbf'  # Kernel type\n",
    "GAMMA = 'scale'  # Kernel coefficient\n",
    "NUM_WORKERS = 4 \n",
    "\n",
    "# ******** 3. DATA PREPARATION ********\n",
    "def get_transforms():\n",
    "    \"\"\"Define image transformations for feature extraction\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return transform\n",
    "\n",
    "def load_data(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
    "    \"\"\"Load and prepare the dataset with train/val/test splits\"\"\"\n",
    "    transform = get_transforms()\n",
    "    \n",
    "    try:\n",
    "        # Load the full dataset\n",
    "        full_dataset = datasets.ImageFolder(DATASET_PATH, transform=transform)\n",
    "        class_names = full_dataset.classes\n",
    "        num_classes = len(class_names)\n",
    "        \n",
    "        # Calculate sizes for splits (70% train, 20% val, 10% test)\n",
    "        total_size = len(full_dataset)\n",
    "        train_size = int(0.7 * total_size)\n",
    "        val_size = int(0.2 * total_size)\n",
    "        test_size = total_size - train_size - val_size\n",
    "        \n",
    "        # Split the dataset\n",
    "        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "            full_dataset, [train_size, val_size, test_size]\n",
    "        )\n",
    "        \n",
    "        # Create data loaders with optimized settings for feature extraction\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False,  # No need to shuffle for feature extraction\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if device.type != 'cpu' else False\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if device.type != 'cpu' else False\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if device.type != 'cpu' else False\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset loaded: {len(train_dataset)} training, {len(val_dataset)} validation, {len(test_dataset)} test images\")\n",
    "        print(f\"Classes: {class_names}\")\n",
    "        \n",
    "        # Print dataset distribution\n",
    "        class_counts = {class_name: 0 for class_name in class_names}\n",
    "        for _, class_idx in full_dataset.samples:\n",
    "            class_counts[class_names[class_idx]] += 1\n",
    "            \n",
    "        print(\"Class distribution:\")\n",
    "        for class_name, count in class_counts.items():\n",
    "            print(f\"  {class_name}: {count} images ({count/total_size:.1%})\")\n",
    "        \n",
    "        return train_loader, val_loader, test_loader, class_names, num_classes\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "# ******** 4. FEATURE EXTRACTION ********\n",
    "def create_feature_extractor(model_name=FEATURE_EXTRACTOR):\n",
    "    \"\"\"Create a model for feature extraction\"\"\"\n",
    "    try:\n",
    "        if model_name == 'resnet50':\n",
    "            model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "            # Remove the classification layer\n",
    "            model = nn.Sequential(*list(model.children())[:-1])\n",
    "        elif model_name == 'efficientnet_v2_s':\n",
    "            model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
    "            # Remove the classification layer\n",
    "            model = nn.Sequential(*list(model.children())[:-1])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "        \n",
    "        # Set to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Move to appropriate device\n",
    "        model = model.to(device)\n",
    "        \n",
    "        print(f\"Feature extractor created: {model_name}\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating feature extractor: {e}\")\n",
    "        raise\n",
    "\n",
    "def extract_features(model, data_loader):\n",
    "    \"\"\"Extract features from images using the feature extractor model\"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(data_loader, desc=\"Extracting features\"):\n",
    "            inputs = inputs.to(device)\n",
    "            # Forward pass to get features\n",
    "            output = model(inputs)\n",
    "            # Flatten the features\n",
    "            output = output.view(output.size(0), -1)\n",
    "            # Move to CPU and convert to numpy\n",
    "            features.append(output.cpu().numpy())\n",
    "            labels.append(targets.numpy())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    features = np.vstack(features)\n",
    "    labels = np.concatenate(labels)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "# ******** 5. MODEL TRAINING ********\n",
    "def train_svm(X_train, y_train, C=C, kernel=KERNEL, gamma=GAMMA):\n",
    "    \"\"\"Train an SVM classifier\"\"\"\n",
    "    print(f\"Training SVM with C={C}, kernel={kernel}, gamma={gamma}\")\n",
    "    \n",
    "    try:\n",
    "        # Create SVM model\n",
    "        svm = SVC(C=C, kernel=kernel, gamma=gamma, probability=True, verbose=False)\n",
    "        \n",
    "        # Train the model\n",
    "        start_time = time.time()\n",
    "        svm.fit(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"SVM training completed in {training_time:.2f} seconds\")\n",
    "        \n",
    "        return svm\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error training SVM: {e}\")\n",
    "        raise\n",
    "\n",
    "# ******** 6. EVALUATION FUNCTION ********\n",
    "def evaluate_model(model, X, y, class_names, save_path=None):\n",
    "    \"\"\"Evaluate the SVM model and generate detailed metrics and visualizations\"\"\"\n",
    "    try:\n",
    "        # Predict\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        report = classification_report(y, y_pred, target_names=class_names, output_dict=True)\n",
    "        report_str = classification_report(y, y_pred, target_names=class_names)\n",
    "        f1 = f1_score(y, y_pred, average='weighted')\n",
    "        \n",
    "        # Print report\n",
    "        print(f'Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}')\n",
    "        print('\\nClassification Report:')\n",
    "        print(report_str)\n",
    "        \n",
    "        if save_path:\n",
    "            # Save detailed report\n",
    "            with open(f\"{save_path}_report.txt\", 'w') as f:\n",
    "                f.write(f'Accuracy: {accuracy:.4f}\\n')\n",
    "                f.write(f'F1 Score: {f1:.4f}\\n\\n')\n",
    "                f.write(report_str)\n",
    "                \n",
    "            # Save report as JSON for further analysis\n",
    "            with open(f\"{save_path}_report.json\", 'w') as f:\n",
    "                json.dump(report, f, indent=4)\n",
    "            \n",
    "            # Plot confusion matrix\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{save_path}_confusion_matrix.png\")\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot normalized confusion matrix\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "            plt.title('Normalized Confusion Matrix')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{save_path}_confusion_matrix_norm.png\")\n",
    "            plt.close()\n",
    "            \n",
    "            print(f'Evaluation results saved to {save_path}')\n",
    "        \n",
    "        return accuracy, report\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating model: {e}\")\n",
    "        raise\n",
    "\n",
    "# ******** 7. MAIN EXECUTION FUNCTION ********\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SVM Baseline Model for Recycling Material Classification\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create directory for baseline results\n",
    "    baseline_dir = os.path.join(RESULTS_DIR, \"baseline\")\n",
    "    os.makedirs(baseline_dir, exist_ok=True)\n",
    "    \n",
    "    # Load data\n",
    "    train_loader, val_loader, test_loader, class_names, num_classes = load_data()\n",
    "    \n",
    "    # Create feature extractor\n",
    "    feature_extractor = create_feature_extractor(model_name=FEATURE_EXTRACTOR)\n",
    "    \n",
    "    # Extract features\n",
    "    print(\"\\nExtracting features from training data...\")\n",
    "    X_train, y_train = extract_features(feature_extractor, train_loader)\n",
    "    print(f\"Training features shape: {X_train.shape}, Labels shape: {y_train.shape}\")\n",
    "    \n",
    "    print(\"\\nExtracting features from validation data...\")\n",
    "    X_val, y_val = extract_features(feature_extractor, val_loader)\n",
    "    print(f\"Validation features shape: {X_val.shape}, Labels shape: {y_val.shape}\")\n",
    "    \n",
    "    print(\"\\nExtracting features from test data...\")\n",
    "    X_test, y_test = extract_features(feature_extractor, test_loader)\n",
    "    print(f\"Test features shape: {X_test.shape}, Labels shape: {y_test.shape}\")\n",
    "    \n",
    "    # Train SVM model\n",
    "    print(\"\\nTraining SVM model...\")\n",
    "    svm_model = train_svm(X_train, y_train, C=C, kernel=KERNEL, gamma=GAMMA)\n",
    "    \n",
    "    # Save the trained model\n",
    "    model_path = os.path.join(baseline_dir, 'svm_model.pkl')\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(svm_model, f)\n",
    "    print(f\"SVM model saved to {model_path}\")\n",
    "    \n",
    "    # Save configuration \n",
    "    config = {\n",
    "        'model': 'SVM',\n",
    "        'feature_extractor': FEATURE_EXTRACTOR,\n",
    "        'C': C,\n",
    "        'kernel': KERNEL,\n",
    "        'gamma': GAMMA,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'num_workers': NUM_WORKERS,\n",
    "        'device': str(device),\n",
    "        'dataset_path': DATASET_PATH,\n",
    "        'num_classes': num_classes,\n",
    "        'class_names': class_names,\n",
    "        'feature_dimension': X_train.shape[1]\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(baseline_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(\"\\nEvaluating on validation set:\")\n",
    "    val_results_path = os.path.join(baseline_dir, 'validation_results')\n",
    "    val_accuracy, val_report = evaluate_model(svm_model, X_val, y_val, class_names, save_path=val_results_path)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set:\")\n",
    "    test_results_path = os.path.join(baseline_dir, 'test_results')\n",
    "    test_accuracy, test_report = evaluate_model(svm_model, X_test, y_test, class_names, save_path=test_results_path)\n",
    "    \n",
    "    # Save model summary\n",
    "    model_summary = {\n",
    "        'model_type': f'SVM with {FEATURE_EXTRACTOR} features',\n",
    "        'feature_extractor': FEATURE_EXTRACTOR,\n",
    "        'num_classes': num_classes,\n",
    "        'class_names': class_names,\n",
    "        'hyperparameters': {\n",
    "            'C': C,\n",
    "            'kernel': KERNEL,\n",
    "            'gamma': GAMMA\n",
    "        },\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'val_f1_score': val_report['weighted avg']['f1-score'],\n",
    "        'test_f1_score': test_report['weighted avg']['f1-score'],\n",
    "        'per_class_f1': {cls: test_report[cls]['f1-score'] for cls in class_names}\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(baseline_dir, 'model_summary.json'), 'w') as f:\n",
    "        json.dump(model_summary, f, indent=4)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"BASELINE MODEL SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Model: SVM with {FEATURE_EXTRACTOR} features\")\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    print(f\"Parameters: C={C}, kernel={KERNEL}, gamma={GAMMA}\")\n",
    "    print(f\"Feature extractor: {FEATURE_EXTRACTOR}\")\n",
    "    print(f\"Feature dimension: {X_train.shape[1]}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Validation F1 Score: {val_report['weighted avg']['f1-score']:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_report['weighted avg']['f1-score']:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Full results saved to {baseline_dir}\")\n",
    "    \n",
    "    return svm_model, test_accuracy, test_report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
