{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ade532-8522-46f3-8cbd-8274b1c760b9",
   "metadata": {},
   "source": [
    "### ******** 1. Import Libraries ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba856675-a6e5-44ab-8d46-c76a3cda6d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8bb38b-e441-49e5-96fe-a969928c545c",
   "metadata": {},
   "source": [
    "### ******** 2. SETUP ENVIRONMENT ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2e7ce5e-cf5e-467d-9555-eaec52db92f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0\n",
      "Using MPS device\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a196edc-2d54-43ad-9c09-4020b2a7ce56",
   "metadata": {},
   "source": [
    "### ******** 3. CONFIGURATION SETTINGS ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6838dc-4eac-4d53-9f27-708cb62453bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATASET_PATH = \"../../dataset-resized\" \n",
    "RESULTS_DIR = \"efficientnet_v2_trashnet_baseline_results\"\n",
    "\n",
    "# DATASET_PATH = \"../../garbage-dataset-2\"  \n",
    "# RESULTS_DIR = \"efficientV2_baseline_garbage_dataset_results\"\n",
    "\n",
    "# DATASET_PATH = \"../../archive/images/images\" \n",
    "# RESULTS_DIR = \"efficientnet_v2_baseline_results_kaggle_dataset\"\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    raise FileNotFoundError(f\"Dataset path {DATASET_PATH} does not exist\")\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "OPTIMIZER = 'Adam'\n",
    "EPOCHS = 30\n",
    "DROPOUT_RATE = 0.5\n",
    "UNFREEZE_LAYERS = 'none'\n",
    "NUM_WORKERS = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8ec29a4-2de4-49f6-bbd5-150d856dc040",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ******** 3. DATA PREPARATION ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52e117ec-6e01-4de1-bc06-19430c6cc316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    \"\"\"Define image transformations for training and validation\"\"\"\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((240, 240)),\n",
    "        transforms.CenterCrop((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),  \n",
    "        transforms.RandomRotation(10),      \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "def load_data(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
    "    \"\"\"Load and prepare the dataset with train/val/test splits\"\"\"\n",
    "    train_transform, val_transform = get_transforms()\n",
    "    \n",
    "    try:\n",
    "        full_dataset = datasets.ImageFolder(DATASET_PATH, transform=train_transform)\n",
    "        class_names = full_dataset.classes\n",
    "        num_classes = len(class_names)\n",
    "        \n",
    "        total_size = len(full_dataset)\n",
    "        train_size = int(0.7 * total_size)\n",
    "        val_size = int(0.2 * total_size)\n",
    "        test_size = total_size - train_size - val_size\n",
    "        \n",
    "        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "            full_dataset, [train_size, val_size, test_size]\n",
    "        )\n",
    "        \n",
    "        val_dataset.dataset.transform = val_transform\n",
    "        test_dataset.dataset.transform = val_transform\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if device.type != 'cpu' else False\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if device.type != 'cpu' else False\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if device.type != 'cpu' else False\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset loaded: {len(train_dataset)} training, {len(val_dataset)} validation, {len(test_dataset)} test images\")\n",
    "        print(f\"Classes: {class_names}\")\n",
    "        \n",
    "        # Print dataset distribution\n",
    "        class_counts = {class_name: 0 for class_name in class_names}\n",
    "        for _, class_idx in full_dataset.samples:\n",
    "            class_counts[class_names[class_idx]] += 1\n",
    "            \n",
    "        print(\"Class distribution:\")\n",
    "        for class_name, count in class_counts.items():\n",
    "            print(f\"  {class_name}: {count} images ({count/total_size:.1%})\")\n",
    "        \n",
    "        return train_loader, val_loader, test_loader, class_names, num_classes\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc52b99-575f-4901-9fa5-ea91a4c28704",
   "metadata": {},
   "source": [
    "### ******** 5. TRAINING SETUP ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab9baf29-5310-48ab-a28e-709df42eca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    def __init__(self, patience=7, verbose=True, path='best_model.pth', delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.path = path\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "    \n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e992686b-b9d1-46e9-b2b2-723b2414b71a",
   "metadata": {},
   "source": [
    "### ******** 6. TRAINING FUNCTION ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc2d4364-a799-4fa9-aafa-91bb4c995e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=EPOCHS, path='model.pth'):\n",
    "    \"\"\"Train the model and track performance metrics\"\"\"\n",
    "    early_stopping = EarlyStopping(patience=7, path=path)\n",
    "    \n",
    "    # History tracking\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [], \n",
    "        'train_acc': [], 'val_acc': [], \n",
    "        'f1': []\n",
    "    }\n",
    "    \n",
    "    model = model.to(device)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * batch_size\n",
    "            running_corrects += torch.sum(preds == labels.data).item()\n",
    "            total_samples += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = running_corrects / total_samples\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc)\n",
    "        \n",
    "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total_samples = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=f\"Validation\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                batch_size = inputs.size(0)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item() * batch_size\n",
    "                running_corrects += torch.sum(preds == labels.data).item()\n",
    "                total_samples += batch_size\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = running_corrects / total_samples\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        history['val_loss'].append(epoch_loss)\n",
    "        history['val_acc'].append(epoch_acc)\n",
    "        history['f1'].append(f1)\n",
    "        \n",
    "        print(f'Val Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} F1: {f1:.4f}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        early_stopping(epoch_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    # Calculate training time\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print(f'Training completed in {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s')\n",
    "    \n",
    "    # Load best model\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(early_stopping.path))\n",
    "        print(f\"Loaded best model from {early_stopping.path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load best model - {e}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "    print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "    print('\\nClassification Report:')\n",
    "    print(report_str)\n",
    "    \n",
    "    if save_path:\n",
    "        # Save detailed report\n",
    "        with open(f\"{save_path}_report.txt\", 'w') as f:\n",
    "            f.write(f'Validation Accuracy: {accuracy:.4f}\\n\\n')\n",
    "            f.write(report_str)\n",
    "            \n",
    "        # Save report as JSON for further analysis\n",
    "        with open(f\"{save_path}_report.json\", 'w') as f:\n",
    "            json.dump(report, f, indent=4)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{save_path}_confusion_matrix.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot normalized confusion matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Normalized Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{save_path}_confusion_matrix_norm.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        print(f'Evaluation results saved to {save_path}')\n",
    "    \n",
    "    return accuracy, report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8536b48-c771-4e77-94fa-f44000813594",
   "metadata": {},
   "source": [
    "### ******** 7. EVALUATION FUNCTION ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e053947-9eeb-403e-85bf-94554d52eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, class_names, save_path=None):\n",
    "    \"\"\"Evaluate the model and generate detailed metrics and visualizations\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
    "    report_str = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "    \n",
    "    # Print report\n",
    "    print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "    print('\\nClassification Report:')\n",
    "    print(report_str)\n",
    "    \n",
    "    if save_path:\n",
    "        # Save detailed report\n",
    "        with open(f\"{save_path}_report.txt\", 'w') as f:\n",
    "            f.write(f'Validation Accuracy: {accuracy:.4f}\\n\\n')\n",
    "            f.write(report_str)\n",
    "            \n",
    "        # Save report as JSON for further analysis\n",
    "        with open(f\"{save_path}_report.json\", 'w') as f:\n",
    "            json.dump(report, f, indent=4)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{save_path}_confusion_matrix.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot normalized confusion matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Normalized Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{save_path}_confusion_matrix_norm.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        print(f'Evaluation results saved to {save_path}')\n",
    "    \n",
    "    return accuracy, report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c091e5-ef65-4bcd-bbc3-9c3285910c30",
   "metadata": {},
   "source": [
    "### ******** 8. VISUALIZATION FUNCTION ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "091808dc-a4c5-4375-97f9-4f93e54cdef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, save_path=None):\n",
    "    \"\"\"Plot training metrics history\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Acc')\n",
    "    plt.plot(history['val_acc'], label='Val Acc')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history['f1'], label='F1 Score')\n",
    "    plt.title('F1 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}_history.png\")\n",
    "        print(f'Training history plot saved to {save_path}_history.png')\n",
    "    \n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f607d05-ca78-4641-b431-db9b713253b2",
   "metadata": {},
   "source": [
    "### ******** 9. MAIN EXECUTION FUNCTION ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df526097-54cd-4476-8b7f-9262203192ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EfficientNetV2-S Baseline Model for Recycling Material Classification\n",
      "==================================================\n",
      "Dataset loaded: 1768 training, 505 validation, 254 test images\n",
      "Classes: ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
      "Class distribution:\n",
      "  cardboard: 403 images (15.9%)\n",
      "  glass: 501 images (19.8%)\n",
      "  metal: 410 images (16.2%)\n",
      "  paper: 594 images (23.5%)\n",
      "  plastic: 482 images (19.1%)\n",
      "  trash: 137 images (5.4%)\n",
      "Model created: EfficientNetV2-S with 6 output classes\n",
      "Total parameters: 20,185,174\n",
      "Trainable parameters: 7,686 (0.04%)\n",
      "Training strategy: Only classifier trained, backbone frozen\n",
      "\n",
      "Starting model training...\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:33<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3574 Acc: 0.5226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.0315 Acc: 0.7366 F1: 0.7198\n",
      "Validation loss decreased (inf --> 1.031513). Saving model...\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:33<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9149 Acc: 0.7234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.8225 Acc: 0.7644 F1: 0.7545\n",
      "Validation loss decreased (1.031513 --> 0.822516). Saving model...\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:33<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7735 Acc: 0.7607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7247 Acc: 0.7762 F1: 0.7714\n",
      "Validation loss decreased (0.822516 --> 0.724670). Saving model...\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:32<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7191 Acc: 0.7743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:27<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6708 Acc: 0.7861 F1: 0.7827\n",
      "Validation loss decreased (0.724670 --> 0.670817). Saving model...\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:32<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6807 Acc: 0.7749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6338 Acc: 0.8000 F1: 0.7994\n",
      "Validation loss decreased (0.670817 --> 0.633793). Saving model...\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:34<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6573 Acc: 0.7834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5982 Acc: 0.8020 F1: 0.8009\n",
      "Validation loss decreased (0.633793 --> 0.598249). Saving model...\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:35<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6096 Acc: 0.7952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:25<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5810 Acc: 0.8198 F1: 0.8187\n",
      "Validation loss decreased (0.598249 --> 0.580970). Saving model...\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:33<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5917 Acc: 0.8077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5555 Acc: 0.8218 F1: 0.8207\n",
      "Validation loss decreased (0.580970 --> 0.555540). Saving model...\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:33<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5767 Acc: 0.8043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5399 Acc: 0.8218 F1: 0.8215\n",
      "Validation loss decreased (0.555540 --> 0.539905). Saving model...\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:34<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5909 Acc: 0.7941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5553 Acc: 0.8277 F1: 0.8260\n",
      "EarlyStopping counter: 1 out of 7\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:33<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5772 Acc: 0.7907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5310 Acc: 0.8198 F1: 0.8199\n",
      "Validation loss decreased (0.539905 --> 0.531020). Saving model...\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:32<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5627 Acc: 0.8094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5274 Acc: 0.8158 F1: 0.8151\n",
      "Validation loss decreased (0.531020 --> 0.527401). Saving model...\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:33<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5448 Acc: 0.8179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5286 Acc: 0.8079 F1: 0.8081\n",
      "EarlyStopping counter: 1 out of 7\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:33<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5462 Acc: 0.8054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5065 Acc: 0.8139 F1: 0.8150\n",
      "Validation loss decreased (0.527401 --> 0.506510). Saving model...\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:32<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5364 Acc: 0.8207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4918 Acc: 0.8396 F1: 0.8388\n",
      "Validation loss decreased (0.506510 --> 0.491791). Saving model...\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:33<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5288 Acc: 0.8133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4956 Acc: 0.8356 F1: 0.8359\n",
      "EarlyStopping counter: 1 out of 7\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:32<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5283 Acc: 0.8150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5024 Acc: 0.8277 F1: 0.8269\n",
      "EarlyStopping counter: 2 out of 7\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:32<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5227 Acc: 0.8128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4884 Acc: 0.8455 F1: 0.8445\n",
      "Validation loss decreased (0.491791 --> 0.488369). Saving model...\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:32<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5107 Acc: 0.8111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5028 Acc: 0.8218 F1: 0.8200\n",
      "EarlyStopping counter: 1 out of 7\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:33<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4977 Acc: 0.8252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4883 Acc: 0.8277 F1: 0.8273\n",
      "Validation loss decreased (0.488369 --> 0.488349). Saving model...\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:34<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5216 Acc: 0.8201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:25<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4696 Acc: 0.8376 F1: 0.8369\n",
      "Validation loss decreased (0.488349 --> 0.469641). Saving model...\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:32<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5413 Acc: 0.8133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4770 Acc: 0.8218 F1: 0.8207\n",
      "EarlyStopping counter: 1 out of 7\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:32<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5033 Acc: 0.8230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4770 Acc: 0.8416 F1: 0.8408\n",
      "EarlyStopping counter: 2 out of 7\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:34<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5281 Acc: 0.8167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4585 Acc: 0.8455 F1: 0.8450\n",
      "Validation loss decreased (0.469641 --> 0.458477). Saving model...\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:32<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5004 Acc: 0.8235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4746 Acc: 0.8356 F1: 0.8353\n",
      "EarlyStopping counter: 1 out of 7\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:32<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5056 Acc: 0.8230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4859 Acc: 0.8416 F1: 0.8407\n",
      "EarlyStopping counter: 2 out of 7\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:32<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5040 Acc: 0.8196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4726 Acc: 0.8455 F1: 0.8448\n",
      "EarlyStopping counter: 3 out of 7\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:32<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4800 Acc: 0.8286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4584 Acc: 0.8376 F1: 0.8373\n",
      "Validation loss decreased (0.458477 --> 0.458448). Saving model...\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:32<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5111 Acc: 0.8139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4617 Acc: 0.8337 F1: 0.8330\n",
      "EarlyStopping counter: 1 out of 7\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 56/56 [00:32<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4983 Acc: 0.8247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4640 Acc: 0.8416 F1: 0.8414\n",
      "EarlyStopping counter: 2 out of 7\n",
      "Training completed in 29m 4s\n",
      "Loaded best model from efficientnet_v2_baseline_results/baseline/best_model.pth\n",
      "Training history plot saved to efficientnet_v2_baseline_results/baseline/training_history_history.png\n",
      "\n",
      "Evaluating on validation set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████| 16/16 [00:24<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8376\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.91      0.91      0.91        75\n",
      "       glass       0.78      0.83      0.80        94\n",
      "       metal       0.81      0.79      0.80        82\n",
      "       paper       0.89      0.91      0.90       120\n",
      "     plastic       0.83      0.77      0.80       102\n",
      "       trash       0.75      0.75      0.75        32\n",
      "\n",
      "    accuracy                           0.84       505\n",
      "   macro avg       0.83      0.83      0.83       505\n",
      "weighted avg       0.84      0.84      0.84       505\n",
      "\n",
      "Evaluation results saved to efficientnet_v2_baseline_results/baseline/validation_results\n",
      "\n",
      "Evaluating on test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████| 8/8 [00:22<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8150\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.89      0.87      0.88        38\n",
      "       glass       0.79      0.80      0.80        51\n",
      "       metal       0.85      0.85      0.85        47\n",
      "       paper       0.87      0.84      0.85        55\n",
      "     plastic       0.74      0.79      0.76        47\n",
      "       trash       0.67      0.62      0.65        16\n",
      "\n",
      "    accuracy                           0.81       254\n",
      "   macro avg       0.80      0.80      0.80       254\n",
      "weighted avg       0.82      0.81      0.82       254\n",
      "\n",
      "Evaluation results saved to efficientnet_v2_baseline_results/baseline/test_results\n",
      "\n",
      "==================================================\n",
      "BASELINE MODEL SUMMARY\n",
      "==================================================\n",
      "Model: EfficientNetV2-S\n",
      "Classes: ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
      "Total Parameters: 20,185,174\n",
      "Trainable Parameters: 7,686\n",
      "Validation Accuracy: 0.8376\n",
      "Test Accuracy: 0.8150\n",
      "Validation F1 Score: 0.8373\n",
      "Test F1 Score: 0.8152\n",
      "==================================================\n",
      "Full results saved to efficientnet_v2_baseline_results/baseline\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EfficientNetV2-S Baseline Model for Recycling Material Classification\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create directory for baseline results\n",
    "    baseline_dir = os.path.join(RESULTS_DIR, \"baseline\")\n",
    "    os.makedirs(baseline_dir, exist_ok=True)\n",
    "    \n",
    "    # Load data\n",
    "    train_loader, val_loader, test_loader, class_names, num_classes = load_data(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_efficientnet_v2(num_classes, dropout_rate=DROPOUT_RATE)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Model created: EfficientNetV2-S with {num_classes} output classes\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%})\")\n",
    "    print(f\"Training strategy: {UNFREEZE_LAYERS if UNFREEZE_LAYERS != 'none' else 'Only classifier trained, backbone frozen'}\")\n",
    "    \n",
    "    # Setup loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if OPTIMIZER == 'Adam':\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                              lr=LEARNING_RATE)\n",
    "    \n",
    "    # Save configuration \n",
    "    config = {\n",
    "        'model': 'EfficientNetV2-S',\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'optimizer': OPTIMIZER,\n",
    "        'epochs': EPOCHS,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'unfreeze_layers': UNFREEZE_LAYERS,\n",
    "        'num_workers': NUM_WORKERS,\n",
    "        'total_parameters': total_params,\n",
    "        'trainable_parameters': trainable_params,\n",
    "        'device': str(device),\n",
    "        'dataset_path': DATASET_PATH,\n",
    "        'num_classes': num_classes,\n",
    "        'class_names': class_names\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(baseline_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"\\nStarting model training...\")\n",
    "    model_path = os.path.join(baseline_dir, 'best_model.pth')\n",
    "    model, history = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, \n",
    "        num_epochs=EPOCHS, path=model_path\n",
    "    )\n",
    "    \n",
    "    # Plot and save training history\n",
    "    history_path = os.path.join(baseline_dir, 'training_history')\n",
    "    plot_history(history, save_path=history_path)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(\"\\nEvaluating on validation set:\")\n",
    "    val_results_path = os.path.join(baseline_dir, 'validation_results')\n",
    "    val_accuracy, val_report = evaluate_model(model, val_loader, class_names, save_path=val_results_path)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set:\")\n",
    "    test_results_path = os.path.join(baseline_dir, 'test_results')\n",
    "    test_accuracy, test_report = evaluate_model(model, test_loader, class_names, save_path=test_results_path)\n",
    "    \n",
    "    # Save model summary\n",
    "    model_summary = {\n",
    "        'model_type': 'EfficientNetV2-S',\n",
    "        'num_classes': num_classes,\n",
    "        'class_names': class_names,\n",
    "        'parameters': total_params,\n",
    "        'trainable_parameters': trainable_params,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'val_f1_score': val_report['weighted avg']['f1-score'],\n",
    "        'test_f1_score': test_report['weighted avg']['f1-score'],\n",
    "        'per_class_f1': {cls: test_report[cls]['f1-score'] for cls in class_names}\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(baseline_dir, 'model_summary.json'), 'w') as f:\n",
    "        json.dump(model_summary, f, indent=4)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"BASELINE MODEL SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Model: EfficientNetV2-S\")\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    print(f\"Total Parameters: {model_summary['parameters']:,}\")\n",
    "    print(f\"Trainable Parameters: {model_summary['trainable_parameters']:,}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Validation F1 Score: {val_report['weighted avg']['f1-score']:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_report['weighted avg']['f1-score']:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Full results saved to {baseline_dir}\")\n",
    "    \n",
    "    return model, test_accuracy, test_report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087eccaf-e128-4f2d-ab2d-027f37bd1283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b6fec3-8442-4f39-b2a4-a1e5d5c33742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
