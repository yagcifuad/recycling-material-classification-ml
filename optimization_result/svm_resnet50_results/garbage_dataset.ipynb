{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c90a278-02c5-412a-8a3a-308122ed78e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SVM Hyperparameter Optimization for Recycling Material Classification\n",
      "==================================================\n",
      "PyTorch version: 2.6.0\n",
      "Using MPS device for feature extraction\n",
      "Dataset loaded: 10500 training, 3000 validation, 1500 test images\n",
      "Classes: ['aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging', 'clothing', 'coffee_grounds', 'disposable_plastic_cutlery', 'eggshells', 'food_waste', 'glass_beverage_bottles', 'glass_cosmetic_containers', 'glass_food_jars', 'magazines', 'newspaper', 'office_paper', 'paper_cups', 'plastic_cup_lids', 'plastic_detergent_bottles', 'plastic_food_containers', 'plastic_shopping_bags', 'plastic_soda_bottles', 'plastic_straws', 'plastic_trash_bags', 'plastic_water_bottles', 'shoes', 'steel_food_cans', 'styrofoam_cups', 'styrofoam_food_containers', 'tea_bags']\n",
      "Class distribution:\n",
      "  aerosol_cans: 500 images (3.3%)\n",
      "  aluminum_food_cans: 500 images (3.3%)\n",
      "  aluminum_soda_cans: 500 images (3.3%)\n",
      "  cardboard_boxes: 500 images (3.3%)\n",
      "  cardboard_packaging: 500 images (3.3%)\n",
      "  clothing: 500 images (3.3%)\n",
      "  coffee_grounds: 500 images (3.3%)\n",
      "  disposable_plastic_cutlery: 500 images (3.3%)\n",
      "  eggshells: 500 images (3.3%)\n",
      "  food_waste: 500 images (3.3%)\n",
      "  glass_beverage_bottles: 500 images (3.3%)\n",
      "  glass_cosmetic_containers: 500 images (3.3%)\n",
      "  glass_food_jars: 500 images (3.3%)\n",
      "  magazines: 500 images (3.3%)\n",
      "  newspaper: 500 images (3.3%)\n",
      "  office_paper: 500 images (3.3%)\n",
      "  paper_cups: 500 images (3.3%)\n",
      "  plastic_cup_lids: 500 images (3.3%)\n",
      "  plastic_detergent_bottles: 500 images (3.3%)\n",
      "  plastic_food_containers: 500 images (3.3%)\n",
      "  plastic_shopping_bags: 500 images (3.3%)\n",
      "  plastic_soda_bottles: 500 images (3.3%)\n",
      "  plastic_straws: 500 images (3.3%)\n",
      "  plastic_trash_bags: 500 images (3.3%)\n",
      "  plastic_water_bottles: 500 images (3.3%)\n",
      "  shoes: 500 images (3.3%)\n",
      "  steel_food_cans: 500 images (3.3%)\n",
      "  styrofoam_cups: 500 images (3.3%)\n",
      "  styrofoam_food_containers: 500 images (3.3%)\n",
      "  tea_bags: 500 images (3.3%)\n",
      "Feature extractor created: resnet50\n",
      "\n",
      "Extracting features from training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|████████████████████| 329/329 [01:30<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (10500, 2048), Labels shape: (10500,)\n",
      "\n",
      "Extracting features from validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████████████████| 94/94 [00:42<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation features shape: (3000, 2048), Labels shape: (3000,)\n",
      "\n",
      "Extracting features from test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████████████████| 47/47 [00:33<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test features shape: (1500, 2048), Labels shape: (1500,)\n",
      "\n",
      "Performing SVM hyperparameter optimization...\n",
      "Starting SVM hyperparameter optimization with 25 iterations...\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.004281332398719396, class_weight=None, decision_function_shape=ovr, gamma=0.8858667904100823, kernel=rbf; total time=20.1min\n",
      "[CV] END C=2.976351441631316, class_weight=None, decision_function_shape=ovo, gamma=0.0006158482110660267, kernel=rbf; total time= 4.7min\n",
      "[CV] END C=0.6951927961775606, class_weight=None, decision_function_shape=ovr, gamma=0.26366508987303583, kernel=rbf; total time=18.9min\n",
      "[CV] END C=12.742749857031322, class_weight=None, decision_function_shape=ovo, gamma=0.00379269019073225, kernel=poly; total time= 5.9min\n",
      "[CV] END C=1000.0, class_weight=balanced, decision_function_shape=ovo, gamma=2.9763514416313193, kernel=sigmoid; total time=17.4min\n",
      "[CV] END C=0.6951927961775606, class_weight=balanced, decision_function_shape=ovr, gamma=0.023357214690901212, kernel=sigmoid; total time= 4.2min\n",
      "[CV] END C=0.001, class_weight=balanced, decision_function_shape=ovo, gamma=0.14384498882876628, kernel=poly; total time= 5.7min\n",
      "[CV] END C=0.0379269019073225, class_weight=balanced, decision_function_shape=ovo, gamma=0.00018329807108324357, kernel=linear; total time= 3.0min\n",
      "[CV] END C=0.1623776739188721, class_weight=balanced, decision_function_shape=ovo, gamma=2.9763514416313193, kernel=sigmoid; total time=17.8min\n",
      "[CV] END C=112.88378916846884, class_weight=None, decision_function_shape=ovo, gamma=0.0069519279617756054, kernel=poly; total time= 5.3min\n",
      "[CV] END C=112.88378916846884, class_weight=balanced, decision_function_shape=ovo, gamma=0.00379269019073225, kernel=poly; total time= 5.6min\n",
      "[CV] END C=112.88378916846884, class_weight=balanced, decision_function_shape=ovr, gamma=2.9763514416313193, kernel=sigmoid; total time=16.9min\n",
      "[CV] END C=233.57214690901213, class_weight=None, decision_function_shape=ovo, gamma=0.26366508987303583, kernel=poly; total time= 5.3min\n",
      "[CV] END C=1000.0, class_weight=None, decision_function_shape=ovr, gamma=0.00379269019073225, kernel=linear; total time= 2.8min\n",
      "[CV] END C=0.00206913808111479, class_weight=balanced, decision_function_shape=ovo, gamma=1.623776739188721, kernel=poly; total time= 5.4min\n",
      "[CV] END C=112.88378916846884, class_weight=None, decision_function_shape=ovr, gamma=0.26366508987303583, kernel=poly; total time= 5.3min\n",
      "[CV] END C=233.57214690901213, class_weight=balanced, decision_function_shape=ovo, gamma=0.4832930238571752, kernel=poly; total time= 5.7min\n",
      "[CV] END C=1.438449888287663, class_weight=None, decision_function_shape=ovo, gamma=0.00018329807108324357, kernel=rbf; total time= 9.4min\n",
      "[CV] END C=0.6951927961775606, class_weight=None, decision_function_shape=ovo, gamma=0.14384498882876628, kernel=sigmoid; total time=15.2min\n",
      "[CV] END C=0.0379269019073225, class_weight=balanced, decision_function_shape=ovo, gamma=0.00018329807108324357, kernel=linear; total time= 3.0min\n",
      "[CV] END C=0.1623776739188721, class_weight=balanced, decision_function_shape=ovo, gamma=2.9763514416313193, kernel=sigmoid; total time=17.9min\n",
      "[CV] END C=0.004281332398719396, class_weight=None, decision_function_shape=ovr, gamma=0.8858667904100823, kernel=rbf; total time=20.0min\n",
      "[CV] END C=2.976351441631316, class_weight=None, decision_function_shape=ovo, gamma=0.0006158482110660267, kernel=rbf; total time= 4.7min\n",
      "[CV] END C=0.6951927961775606, class_weight=None, decision_function_shape=ovr, gamma=0.26366508987303583, kernel=rbf; total time=18.8min\n",
      "[CV] END C=12.742749857031322, class_weight=None, decision_function_shape=ovo, gamma=0.00379269019073225, kernel=poly; total time= 5.8min\n",
      "[CV] END C=1000.0, class_weight=balanced, decision_function_shape=ovo, gamma=2.9763514416313193, kernel=sigmoid; total time=17.6min\n",
      "[CV] END C=0.6951927961775606, class_weight=balanced, decision_function_shape=ovr, gamma=0.023357214690901212, kernel=sigmoid; total time= 4.2min\n",
      "[CV] END C=0.001, class_weight=balanced, decision_function_shape=ovo, gamma=0.14384498882876628, kernel=poly; total time= 5.8min\n",
      "[CV] END C=0.0379269019073225, class_weight=balanced, decision_function_shape=ovo, gamma=0.00018329807108324357, kernel=linear; total time= 3.1min\n",
      "[CV] END C=0.1623776739188721, class_weight=balanced, decision_function_shape=ovo, gamma=2.9763514416313193, kernel=sigmoid; total time=17.9min\n",
      "[CV] END C=112.88378916846884, class_weight=None, decision_function_shape=ovo, gamma=0.0069519279617756054, kernel=poly; total time= 5.4min\n",
      "[CV] END C=112.88378916846884, class_weight=balanced, decision_function_shape=ovo, gamma=0.00379269019073225, kernel=poly; total time= 5.4min\n",
      "[CV] END C=112.88378916846884, class_weight=balanced, decision_function_shape=ovr, gamma=2.9763514416313193, kernel=sigmoid; total time=16.9min\n",
      "[CV] END C=233.57214690901213, class_weight=None, decision_function_shape=ovo, gamma=0.26366508987303583, kernel=poly; total time= 5.4min\n",
      "[CV] END C=1000.0, class_weight=None, decision_function_shape=ovr, gamma=0.00379269019073225, kernel=linear; total time= 2.8min\n",
      "[CV] END C=0.00206913808111479, class_weight=balanced, decision_function_shape=ovo, gamma=1.623776739188721, kernel=poly; total time= 5.4min\n",
      "[CV] END C=112.88378916846884, class_weight=None, decision_function_shape=ovr, gamma=0.26366508987303583, kernel=poly; total time= 5.3min\n",
      "[CV] END C=233.57214690901213, class_weight=balanced, decision_function_shape=ovo, gamma=0.4832930238571752, kernel=poly; total time= 5.7min\n",
      "[CV] END C=1.438449888287663, class_weight=None, decision_function_shape=ovo, gamma=0.00018329807108324357, kernel=rbf; total time= 9.6min\n",
      "[CV] END C=0.6951927961775606, class_weight=None, decision_function_shape=ovo, gamma=0.14384498882876628, kernel=sigmoid; total time=15.2min\n",
      "[CV] END C=0.0379269019073225, class_weight=balanced, decision_function_shape=ovo, gamma=0.00018329807108324357, kernel=linear; total time= 3.1min\n",
      "[CV] END C=0.1623776739188721, class_weight=balanced, decision_function_shape=ovo, gamma=2.9763514416313193, kernel=sigmoid; total time=18.0min\n",
      "[CV] END C=112.88378916846884, class_weight=None, decision_function_shape=ovo, gamma=0.0069519279617756054, kernel=poly; total time= 5.3min\n",
      "[CV] END C=112.88378916846884, class_weight=balanced, decision_function_shape=ovo, gamma=0.00379269019073225, kernel=poly; total time= 5.7min\n",
      "[CV] END C=112.88378916846884, class_weight=balanced, decision_function_shape=ovr, gamma=2.9763514416313193, kernel=sigmoid; total time=17.0min\n",
      "[CV] END C=233.57214690901213, class_weight=None, decision_function_shape=ovo, gamma=0.26366508987303583, kernel=poly; total time= 5.3min\n",
      "[CV] END C=1000.0, class_weight=None, decision_function_shape=ovr, gamma=0.00379269019073225, kernel=linear; total time= 2.7min\n",
      "[CV] END C=0.00206913808111479, class_weight=balanced, decision_function_shape=ovo, gamma=1.623776739188721, kernel=poly; total time= 5.4min\n",
      "[CV] END C=112.88378916846884, class_weight=None, decision_function_shape=ovr, gamma=0.26366508987303583, kernel=poly; total time= 5.2min\n",
      "[CV] END C=233.57214690901213, class_weight=balanced, decision_function_shape=ovo, gamma=0.4832930238571752, kernel=poly; total time= 5.8min\n",
      "[CV] END C=1.438449888287663, class_weight=None, decision_function_shape=ovo, gamma=0.00018329807108324357, kernel=rbf; total time= 9.5min\n",
      "[CV] END C=0.6951927961775606, class_weight=None, decision_function_shape=ovo, gamma=0.14384498882876628, kernel=sigmoid; total time=15.3min\n",
      "[CV] END C=0.0379269019073225, class_weight=balanced, decision_function_shape=ovo, gamma=0.00018329807108324357, kernel=linear; total time= 3.0min\n",
      "[CV] END C=0.1623776739188721, class_weight=balanced, decision_function_shape=ovo, gamma=2.9763514416313193, kernel=sigmoid; total time=17.9min\n",
      "Best parameters found: {'kernel': 'linear', 'gamma': 0.00018329807108324357, 'decision_function_shape': 'ovo', 'class_weight': 'balanced', 'C': 0.0379269019073225}\n",
      "Best cross-validation score: 0.8689\n",
      "Hyperparameter search completed in 6295.91 seconds\n",
      "Validation accuracy with best parameters: 0.8650\n",
      "Validation F1-score with best parameters: 0.8644\n",
      "Hyperparameter search plots saved to svm_hyperopt_results_for_kaggleds/hyperopt/hyperparameter_plots\n",
      "Best SVM model saved to svm_hyperopt_results_for_kaggleds/hyperopt/best_svm_model.pkl\n",
      "\n",
      "Evaluating best model on validation set:\n",
      "Accuracy: 0.8650, F1 Score: 0.8644\n",
      "\n",
      "Classification Report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "              aerosol_cans       0.92      0.87      0.90       109\n",
      "        aluminum_food_cans       0.52      0.45      0.48        96\n",
      "        aluminum_soda_cans       0.76      0.89      0.82       107\n",
      "           cardboard_boxes       0.64      0.73      0.68        92\n",
      "       cardboard_packaging       0.71      0.65      0.68       106\n",
      "                  clothing       0.86      0.87      0.87       102\n",
      "            coffee_grounds       0.97      0.97      0.97       114\n",
      "disposable_plastic_cutlery       0.99      1.00      0.99        84\n",
      "                 eggshells       0.94      0.91      0.92        98\n",
      "                food_waste       0.93      0.98      0.95        90\n",
      "    glass_beverage_bottles       0.84      0.95      0.89       108\n",
      " glass_cosmetic_containers       0.91      0.90      0.91        91\n",
      "           glass_food_jars       0.95      0.89      0.92       101\n",
      "                 magazines       0.88      0.97      0.92        89\n",
      "                 newspaper       0.85      0.87      0.86        89\n",
      "              office_paper       0.88      0.84      0.86        96\n",
      "                paper_cups       0.82      0.88      0.85       108\n",
      "          plastic_cup_lids       0.92      0.83      0.88       102\n",
      " plastic_detergent_bottles       0.97      0.94      0.96       105\n",
      "   plastic_food_containers       0.85      0.82      0.84        91\n",
      "     plastic_shopping_bags       0.94      0.92      0.93       110\n",
      "      plastic_soda_bottles       0.86      0.78      0.82       103\n",
      "            plastic_straws       0.96      0.98      0.97        90\n",
      "        plastic_trash_bags       0.94      0.94      0.94       117\n",
      "     plastic_water_bottles       0.88      0.86      0.87       118\n",
      "                     shoes       0.99      0.97      0.98        87\n",
      "           steel_food_cans       0.62      0.64      0.63       121\n",
      "            styrofoam_cups       0.93      0.87      0.90       100\n",
      " styrofoam_food_containers       0.88      0.97      0.92        87\n",
      "                  tea_bags       0.92      0.88      0.90        89\n",
      "\n",
      "                  accuracy                           0.86      3000\n",
      "                 macro avg       0.87      0.87      0.87      3000\n",
      "              weighted avg       0.87      0.86      0.86      3000\n",
      "\n",
      "Evaluation results saved to svm_hyperopt_results_for_kaggleds/hyperopt/validation_results\n",
      "\n",
      "Evaluating best model on test set:\n",
      "Accuracy: 0.8587, F1 Score: 0.8566\n",
      "\n",
      "Classification Report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "              aerosol_cans       0.91      0.95      0.93        55\n",
      "        aluminum_food_cans       0.61      0.43      0.51        51\n",
      "        aluminum_soda_cans       0.76      0.91      0.83        45\n",
      "           cardboard_boxes       0.61      0.76      0.68        54\n",
      "       cardboard_packaging       0.69      0.48      0.56        50\n",
      "                  clothing       0.85      0.87      0.86        46\n",
      "            coffee_grounds       0.96      0.98      0.97        53\n",
      "disposable_plastic_cutlery       0.96      0.94      0.95        52\n",
      "                 eggshells       0.92      0.98      0.95        47\n",
      "                food_waste       1.00      1.00      1.00        45\n",
      "    glass_beverage_bottles       0.86      0.93      0.89        54\n",
      " glass_cosmetic_containers       0.95      0.95      0.95        44\n",
      "           glass_food_jars       0.94      0.91      0.92        32\n",
      "                 magazines       0.88      0.91      0.89        55\n",
      "                 newspaper       0.78      0.80      0.79        64\n",
      "              office_paper       0.87      0.85      0.86        53\n",
      "                paper_cups       0.80      0.77      0.79        43\n",
      "          plastic_cup_lids       0.86      0.91      0.88        54\n",
      " plastic_detergent_bottles       0.97      0.97      0.97        31\n",
      "   plastic_food_containers       0.86      0.86      0.86        44\n",
      "     plastic_shopping_bags       0.86      0.96      0.91        52\n",
      "      plastic_soda_bottles       0.81      0.78      0.80        55\n",
      "            plastic_straws       0.96      0.88      0.91        49\n",
      "        plastic_trash_bags       0.98      0.95      0.97        60\n",
      "     plastic_water_bottles       0.85      0.74      0.79        46\n",
      "                     shoes       0.98      1.00      0.99        50\n",
      "           steel_food_cans       0.62      0.73      0.67        49\n",
      "            styrofoam_cups       0.90      0.83      0.87        54\n",
      " styrofoam_food_containers       0.83      0.88      0.86        51\n",
      "                  tea_bags       0.98      0.90      0.94        62\n",
      "\n",
      "                  accuracy                           0.86      1500\n",
      "                 macro avg       0.86      0.86      0.86      1500\n",
      "              weighted avg       0.86      0.86      0.86      1500\n",
      "\n",
      "Evaluation results saved to svm_hyperopt_results_for_kaggleds/hyperopt/test_results\n",
      "\n",
      "==================================================\n",
      "OPTIMIZED SVM MODEL SUMMARY\n",
      "==================================================\n",
      "Model: SVM with resnet50 features\n",
      "Classes: ['aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging', 'clothing', 'coffee_grounds', 'disposable_plastic_cutlery', 'eggshells', 'food_waste', 'glass_beverage_bottles', 'glass_cosmetic_containers', 'glass_food_jars', 'magazines', 'newspaper', 'office_paper', 'paper_cups', 'plastic_cup_lids', 'plastic_detergent_bottles', 'plastic_food_containers', 'plastic_shopping_bags', 'plastic_soda_bottles', 'plastic_straws', 'plastic_trash_bags', 'plastic_water_bottles', 'shoes', 'steel_food_cans', 'styrofoam_cups', 'styrofoam_food_containers', 'tea_bags']\n",
      "Best Parameters: {'C': 0.0379269019073225, 'break_ties': False, 'cache_size': 200, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovo', 'degree': 3, 'gamma': 0.00018329807108324357, 'kernel': 'linear', 'max_iter': -1, 'probability': True, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "Feature extractor: resnet50\n",
      "Feature dimension: 2048\n",
      "Validation Accuracy: 0.8650\n",
      "Test Accuracy: 0.8587\n",
      "Validation F1 Score: 0.8644\n",
      "Test F1 Score: 0.8566\n",
      "==================================================\n",
      "Full results saved to svm_hyperopt_results_for_kaggleds/hyperopt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define paths\n",
    "DATASET_PATH = \"../archive/images/images\"  # Path to your dataset\n",
    "RESULTS_DIR = \"svm_hyperopt_results_for_kaggleds\"\n",
    "\n",
    "# Check if dataset path exists\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    raise FileNotFoundError(f\"Dataset path {DATASET_PATH} does not exist\")\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Define feature extraction settings\n",
    "BATCH_SIZE = 32\n",
    "FEATURE_EXTRACTOR = 'resnet50'  # Using ResNet-50 as feature extractor\n",
    "NUM_WORKERS = 4  # Adjust based on your CPU\n",
    "\n",
    "# Function to check device availability\n",
    "def get_device():\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS device for feature extraction\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"Using CUDA device for feature extraction: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU device for feature extraction\")\n",
    "    return device\n",
    "\n",
    "# Data preprocessing and loading functions\n",
    "def get_transforms():\n",
    "    \"\"\"Define image transformations for feature extraction\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return transform\n",
    "\n",
    "def load_data(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, device=None):\n",
    "    \"\"\"Load and prepare the dataset with train/val/test splits\"\"\"\n",
    "    transform = get_transforms()\n",
    "    \n",
    "    try:\n",
    "        # Load the full dataset\n",
    "        full_dataset = datasets.ImageFolder(DATASET_PATH, transform=transform)\n",
    "        class_names = full_dataset.classes\n",
    "        num_classes = len(class_names)\n",
    "        \n",
    "        # Calculate sizes for splits (70% train, 20% val, 10% test)\n",
    "        total_size = len(full_dataset)\n",
    "        train_size = int(0.7 * total_size)\n",
    "        val_size = int(0.2 * total_size)\n",
    "        test_size = total_size - train_size - val_size\n",
    "        \n",
    "        # Split the dataset\n",
    "        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "            full_dataset, [train_size, val_size, test_size],\n",
    "            generator=torch.Generator().manual_seed(42)  # Ensure reproducibility\n",
    "        )\n",
    "        \n",
    "        # Set pin_memory based on device\n",
    "        pin_memory = False\n",
    "        if device is not None and device.type != 'cpu':\n",
    "            pin_memory = True\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False,  # No need to shuffle for feature extraction\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset loaded: {len(train_dataset)} training, {len(val_dataset)} validation, {len(test_dataset)} test images\")\n",
    "        print(f\"Classes: {class_names}\")\n",
    "        \n",
    "        # Print dataset distribution\n",
    "        class_counts = {class_name: 0 for class_name in class_names}\n",
    "        for _, class_idx in full_dataset.samples:\n",
    "            class_counts[class_names[class_idx]] += 1\n",
    "            \n",
    "        print(\"Class distribution:\")\n",
    "        for class_name, count in class_counts.items():\n",
    "            print(f\"  {class_name}: {count} images ({count/total_size:.1%})\")\n",
    "        \n",
    "        return train_loader, val_loader, test_loader, class_names, num_classes\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "# Feature extraction functions\n",
    "def create_feature_extractor(model_name=FEATURE_EXTRACTOR, device=None):\n",
    "    \"\"\"Create a model for feature extraction\"\"\"\n",
    "    try:\n",
    "        if model_name == 'resnet50':\n",
    "            model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "            # Remove the classification layer\n",
    "            model = nn.Sequential(*list(model.children())[:-1])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "        \n",
    "        # Set to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Move to appropriate device\n",
    "        if device is not None:\n",
    "            model = model.to(device)\n",
    "        \n",
    "        print(f\"Feature extractor created: {model_name}\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating feature extractor: {e}\")\n",
    "        raise\n",
    "\n",
    "def extract_features(model, data_loader, device):\n",
    "    \"\"\"Extract features from images using the feature extractor model\"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(data_loader, desc=\"Extracting features\"):\n",
    "            inputs = inputs.to(device)\n",
    "            # Forward pass to get features\n",
    "            output = model(inputs)\n",
    "            # Flatten the features\n",
    "            output = output.view(output.size(0), -1)\n",
    "            # Move to CPU and convert to numpy\n",
    "            features.append(output.cpu().numpy())\n",
    "            labels.append(targets.numpy())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    features = np.vstack(features)\n",
    "    labels = np.concatenate(labels)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "# SVM hyperparameter optimization\n",
    "def optimize_svm_hyperparameters(X_train, y_train, X_val, y_val, cv=5, n_iter=25):\n",
    "    \"\"\"Optimize SVM hyperparameters using RandomizedSearchCV with cross-validation\"\"\"\n",
    "    print(f\"Starting SVM hyperparameter optimization with {n_iter} iterations...\")\n",
    "    \n",
    "    # Define hyperparameter space to search\n",
    "    param_grid = {\n",
    "        'C': np.logspace(-3, 3, 20),  # Regularization parameter\n",
    "        'gamma': np.logspace(-4, 1, 20),  # Kernel coefficient\n",
    "        'kernel': ['rbf', 'linear', 'poly', 'sigmoid'],\n",
    "        'class_weight': ['balanced', None],\n",
    "        'decision_function_shape': ['ovr', 'ovo'],\n",
    "    }\n",
    "    \n",
    "    # Create base SVM model\n",
    "    svm = SVC(probability=True, random_state=42)\n",
    "    \n",
    "    # Use RandomizedSearchCV for efficient search\n",
    "    # Note: This uses cross-validation on the training set\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=svm,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,  # Use all available cores\n",
    "        verbose=2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Fit the random search on training data\n",
    "    start_time = time.time()\n",
    "    random_search.fit(X_train, y_train)\n",
    "    search_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Best parameters found: {random_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {random_search.best_score_:.4f}\")\n",
    "    print(f\"Hyperparameter search completed in {search_time:.2f} seconds\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    best_svm = random_search.best_estimator_\n",
    "    val_accuracy = best_svm.score(X_val, y_val)\n",
    "    y_val_pred = best_svm.predict(X_val)\n",
    "    val_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "    \n",
    "    print(f\"Validation accuracy with best parameters: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation F1-score with best parameters: {val_f1:.4f}\")\n",
    "    \n",
    "    # Create and save search results summary\n",
    "    results = {\n",
    "        'best_params': random_search.best_params_,\n",
    "        'best_cv_score': float(random_search.best_score_),\n",
    "        'validation_accuracy': float(val_accuracy),\n",
    "        'validation_f1': float(val_f1),\n",
    "        'search_time': search_time,\n",
    "        'cv_results': pd.DataFrame(random_search.cv_results_).to_dict()\n",
    "    }\n",
    "    \n",
    "    # Create a DataFrame of the results for easier visualization\n",
    "    cv_results_df = pd.DataFrame(random_search.cv_results_)\n",
    "    \n",
    "    return best_svm, results, cv_results_df\n",
    "\n",
    "# Evaluation Function for SVM\n",
    "def evaluate_model(model, X, y, class_names, save_path=None):\n",
    "    \"\"\"Evaluate the SVM model and generate detailed metrics and visualizations\"\"\"\n",
    "    try:\n",
    "        # Predict\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        report = classification_report(y, y_pred, target_names=class_names, output_dict=True)\n",
    "        report_str = classification_report(y, y_pred, target_names=class_names)\n",
    "        f1 = f1_score(y, y_pred, average='weighted')\n",
    "        \n",
    "        # Print report\n",
    "        print(f'Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}')\n",
    "        print('\\nClassification Report:')\n",
    "        print(report_str)\n",
    "        \n",
    "        if save_path:\n",
    "            # Save detailed report\n",
    "            with open(f\"{save_path}_report.txt\", 'w') as f:\n",
    "                f.write(f'Accuracy: {accuracy:.4f}\\n')\n",
    "                f.write(f'F1 Score: {f1:.4f}\\n\\n')\n",
    "                f.write(report_str)\n",
    "                \n",
    "            # Save report as JSON for further analysis\n",
    "            with open(f\"{save_path}_report.json\", 'w') as f:\n",
    "                json.dump(report, f, indent=4)\n",
    "            \n",
    "            # Plot confusion matrix\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{save_path}_confusion_matrix.png\")\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot normalized confusion matrix\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "            plt.title('Normalized Confusion Matrix')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{save_path}_confusion_matrix_norm.png\")\n",
    "            plt.close()\n",
    "            \n",
    "            print(f'Evaluation results saved to {save_path}')\n",
    "        \n",
    "        return accuracy, report\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating model: {e}\")\n",
    "        raise\n",
    "\n",
    "# Plot hyperparameter search results\n",
    "def plot_search_results(cv_results_df, save_path):\n",
    "    \"\"\"Plot the results of hyperparameter search for visualization\"\"\"\n",
    "    # Create directory for plots\n",
    "    plots_dir = os.path.join(save_path, 'hyperparameter_plots')\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Plot mean test scores by parameters\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 1.1 C parameter\n",
    "    plt.subplot(2, 2, 1)\n",
    "    # Group by C parameter and calculate mean score\n",
    "    C_scores = cv_results_df.groupby('param_C')['mean_test_score'].mean().reset_index()\n",
    "    C_scores['param_C'] = C_scores['param_C'].astype(float)\n",
    "    C_scores = C_scores.sort_values('param_C')\n",
    "    \n",
    "    plt.semilogx(C_scores['param_C'], C_scores['mean_test_score'], 'o-')\n",
    "    plt.xlabel('C parameter (log scale)')\n",
    "    plt.ylabel('Mean F1 Score')\n",
    "    plt.title('Effect of C Parameter on Performance')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 1.2 Gamma parameter (for non-linear kernels)\n",
    "    plt.subplot(2, 2, 2)\n",
    "    # Filter for non-linear kernels\n",
    "    non_linear_kernels = cv_results_df[cv_results_df['param_kernel'].isin(['rbf', 'poly', 'sigmoid'])]\n",
    "    \n",
    "    if not non_linear_kernels.empty:\n",
    "        # Group by gamma parameter and calculate mean score\n",
    "        gamma_scores = non_linear_kernels.groupby('param_gamma')['mean_test_score'].mean().reset_index()\n",
    "        gamma_scores['param_gamma'] = gamma_scores['param_gamma'].astype(float)\n",
    "        gamma_scores = gamma_scores.sort_values('param_gamma')\n",
    "        \n",
    "        plt.semilogx(gamma_scores['param_gamma'], gamma_scores['mean_test_score'], 'o-')\n",
    "        plt.xlabel('Gamma parameter (log scale)')\n",
    "        plt.ylabel('Mean F1 Score')\n",
    "        plt.title('Effect of Gamma Parameter on Performance')\n",
    "        plt.grid(True)\n",
    "    \n",
    "    # 1.3 Kernel type\n",
    "    plt.subplot(2, 2, 3)\n",
    "    kernel_scores = cv_results_df.groupby('param_kernel')['mean_test_score'].mean().reset_index()\n",
    "    plt.bar(kernel_scores['param_kernel'], kernel_scores['mean_test_score'])\n",
    "    plt.xlabel('Kernel Type')\n",
    "    plt.ylabel('Mean F1 Score')\n",
    "    plt.title('Effect of Kernel Type on Performance')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 1.4 Class weight\n",
    "    plt.subplot(2, 2, 4)\n",
    "    weight_scores = cv_results_df.groupby('param_class_weight')['mean_test_score'].mean().reset_index()\n",
    "    plt.bar(weight_scores['param_class_weight'].astype(str), weight_scores['mean_test_score'])\n",
    "    plt.xlabel('Class Weight')\n",
    "    plt.ylabel('Mean F1 Score')\n",
    "    plt.title('Effect of Class Weight on Performance')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, 'parameter_effects.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Plot top N parameter combinations\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_n = min(20, len(cv_results_df))\n",
    "    top_results = cv_results_df.sort_values('mean_test_score', ascending=False).head(top_n)\n",
    "    \n",
    "    # Create parameter combination labels\n",
    "    param_labels = []\n",
    "    for i, row in top_results.iterrows():\n",
    "        label = f\"C={row['param_C']:.2e}, γ={row['param_gamma']:.2e}, {row['param_kernel'][:3]}\"\n",
    "        param_labels.append(label)\n",
    "    \n",
    "    plt.barh(range(len(top_results)), top_results['mean_test_score'], align='center')\n",
    "    plt.yticks(range(len(top_results)), param_labels)\n",
    "    plt.xlabel('Mean F1 Score')\n",
    "    plt.title(f'Top {top_n} Parameter Combinations')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, 'top_parameters.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Plot score distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(cv_results_df['mean_test_score'], bins=20, alpha=0.7, color='blue')\n",
    "    plt.axvline(cv_results_df['mean_test_score'].max(), color='red', linestyle='dashed', \n",
    "                linewidth=2, label=f'Best score: {cv_results_df[\"mean_test_score\"].max():.4f}')\n",
    "    plt.xlabel('Mean F1 Score')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of F1 Scores')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(plots_dir, 'score_distribution.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Hyperparameter search plots saved to {plots_dir}\")\n",
    "\n",
    "# Main function for SVM hyperparameter optimization\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SVM Hyperparameter Optimization for Recycling Material Classification\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create directory for hyperopt results\n",
    "    hyperopt_dir = os.path.join(RESULTS_DIR, \"hyperopt\")\n",
    "    os.makedirs(hyperopt_dir, exist_ok=True)\n",
    "    \n",
    "    # Get device\n",
    "    device = get_device()\n",
    "    \n",
    "    # Load data - pass device to function\n",
    "    train_loader, val_loader, test_loader, class_names, num_classes = load_data(device=device)\n",
    "    \n",
    "    # Create feature extractor\n",
    "    feature_extractor = create_feature_extractor(model_name=FEATURE_EXTRACTOR, device=device)\n",
    "    \n",
    "    # Extract features for all sets\n",
    "    print(\"\\nExtracting features from training data...\")\n",
    "    X_train, y_train = extract_features(feature_extractor, train_loader, device)\n",
    "    print(f\"Training features shape: {X_train.shape}, Labels shape: {y_train.shape}\")\n",
    "    \n",
    "    print(\"\\nExtracting features from validation data...\")\n",
    "    X_val, y_val = extract_features(feature_extractor, val_loader, device)\n",
    "    print(f\"Validation features shape: {X_val.shape}, Labels shape: {y_val.shape}\")\n",
    "    \n",
    "    print(\"\\nExtracting features from test data...\")\n",
    "    X_test, y_test = extract_features(feature_extractor, test_loader, device)\n",
    "    print(f\"Test features shape: {X_test.shape}, Labels shape: {y_test.shape}\")\n",
    "    \n",
    "    # Perform hyperparameter optimization\n",
    "    print(\"\\nPerforming SVM hyperparameter optimization...\")\n",
    "    best_svm, opt_results, cv_results_df = optimize_svm_hyperparameters(\n",
    "        X_train, y_train, X_val, y_val, cv=5, n_iter=25\n",
    "    )\n",
    "    \n",
    "    # Save hyperparameter search results\n",
    "    with open(os.path.join(hyperopt_dir, 'hyperparameter_search_results.json'), 'w') as f:\n",
    "        # Convert numpy values to Python native types for JSON serialization\n",
    "        results_json = {k: v for k, v in opt_results.items() if k != 'cv_results'}\n",
    "        json.dump(results_json, f, indent=4)\n",
    "    \n",
    "    # Save CV results DataFrame\n",
    "    cv_results_df.to_csv(os.path.join(hyperopt_dir, 'cv_results.csv'), index=False)\n",
    "    \n",
    "    # Plot hyperparameter search results\n",
    "    plot_search_results(cv_results_df, hyperopt_dir)\n",
    "    \n",
    "    # Save the best model\n",
    "    best_model_path = os.path.join(hyperopt_dir, 'best_svm_model.pkl')\n",
    "    with open(best_model_path, 'wb') as f:\n",
    "        pickle.dump(best_svm, f)\n",
    "    print(f\"Best SVM model saved to {best_model_path}\")\n",
    "    \n",
    "    # Save configuration\n",
    "    config = {\n",
    "        'model': 'SVM',\n",
    "        'feature_extractor': FEATURE_EXTRACTOR,\n",
    "        'best_parameters': best_svm.get_params(),\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'num_workers': NUM_WORKERS,\n",
    "        'device': str(device),\n",
    "        'dataset_path': DATASET_PATH,\n",
    "        'num_classes': num_classes,\n",
    "        'class_names': class_names,\n",
    "        'feature_dimension': X_train.shape[1],\n",
    "        'hyperparameter_search': {\n",
    "            'method': 'RandomizedSearchCV',\n",
    "            'n_iter': 25,\n",
    "            'cv': 5,\n",
    "            'scoring': 'f1_weighted'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(hyperopt_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    # Evaluate best model on validation set\n",
    "    print(\"\\nEvaluating best model on validation set:\")\n",
    "    val_results_path = os.path.join(hyperopt_dir, 'validation_results')\n",
    "    val_accuracy, val_report = evaluate_model(best_svm, X_val, y_val, class_names, save_path=val_results_path)\n",
    "    \n",
    "    # Evaluate best model on test set\n",
    "    print(\"\\nEvaluating best model on test set:\")\n",
    "    test_results_path = os.path.join(hyperopt_dir, 'test_results')\n",
    "    test_accuracy, test_report = evaluate_model(best_svm, X_test, y_test, class_names, save_path=test_results_path)\n",
    "    \n",
    "    # Save model summary\n",
    "    model_summary = {\n",
    "        'model_type': f'SVM with {FEATURE_EXTRACTOR} features (optimized)',\n",
    "        'feature_extractor': FEATURE_EXTRACTOR,\n",
    "        'num_classes': num_classes,\n",
    "        'class_names': class_names,\n",
    "        'hyperparameters': best_svm.get_params(),\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'val_f1_score': val_report['weighted avg']['f1-score'],\n",
    "        'test_f1_score': test_report['weighted avg']['f1-score'],\n",
    "        'per_class_f1': {cls: test_report[cls]['f1-score'] for cls in class_names}\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(hyperopt_dir, 'model_summary.json'), 'w') as f:\n",
    "        json.dump(model_summary, f, indent=4)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"OPTIMIZED SVM MODEL SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Model: SVM with {FEATURE_EXTRACTOR} features\")\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    print(f\"Best Parameters: {best_svm.get_params()}\")\n",
    "    print(f\"Feature extractor: {FEATURE_EXTRACTOR}\")\n",
    "    print(f\"Feature dimension: {X_train.shape[1]}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Validation F1 Score: {val_report['weighted avg']['f1-score']:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_report['weighted avg']['f1-score']:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Full results saved to {hyperopt_dir}\")\n",
    "    \n",
    "    return best_svm, test_accuracy, test_report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a7571-5971-4c05-a101-66b9868f8fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
